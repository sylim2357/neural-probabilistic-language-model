{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.negsampling_dataset import NegSamplingDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.corpus import treebank\n",
    "from abc import abstractmethod\n",
    "from nltk import sent_tokenize\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import argparse\n",
    "import easydict\n",
    "import random\n",
    "import pickle\n",
    "import MeCab\n",
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path is C:\\Users\\sylim2357\\AppData\\Roaming\\jupyter\\runtime\\kernel-7edcf06f-a52e-42db-b563-28527c26a594.json\n"
     ]
    }
   ],
   "source": [
    "def config_parser(args):\n",
    "    print('file path is ' + str(args.file_path))\n",
    "    with open(args.config_path, 'rb') as f:\n",
    "        config = easydict.EasyDict(json.load(f))\n",
    "    config.model = args.model\n",
    "    config.file_path = args.file_path\n",
    "    config.dataset_path = args.dataset_path\n",
    "    config.device = torch.device(args.device)\n",
    "    return config\n",
    "\n",
    "args = argparse.ArgumentParser(description='nlp embedding')\n",
    "args.add_argument('-m', '--model', default='neg-sampling', type=str,\n",
    "                  help='which model to use')\n",
    "args.add_argument('-cp', '--config-path', default='config.json', type=str,\n",
    "                  help='config file path (default: None)')\n",
    "args.add_argument('-fp', '--file-path', default='D:\\\\data\\\\text\\\\news-articles\\\\kbanker_articles_subtitles.csv', type=str,\n",
    "                  help='path to latest checkpoint (default: None)')\n",
    "args.add_argument('-dp', '--dataset-path', default='data\\\\nlp_dataset.pkl', type=str,\n",
    "                  help='if there is a pickled dataset')\n",
    "args.add_argument('-d', '--device', default='cuda:0', type=str,\n",
    "                  help='indices of GPUs to enable (default: all)')\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname('__file__'))))\n",
    "config = config_parser(args.parse_args())\n",
    "config['neg_sample_size'] = 5\n",
    "config['file_path'] = 'D:\\\\data\\\\text\\\\torch-dataset\\\\kbanker_articles_processed.pkl'\n",
    "config['file_path'] = 'E:\\\\data\\\\text\\\\news-articles\\\\kbanker_articles_subtitles.csv'\n",
    "config['file_path'] = 'treebank'\n",
    "config['model'] = 'fast-text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(w, n):\n",
    "    word = '<' + w + '>'\n",
    "    if len(word) <= 3:\n",
    "        return [word]\n",
    "    else:\n",
    "        ngram = []\n",
    "        for i in range(n, len(word)+1):\n",
    "            ngram += [word[i-n:i]]\n",
    "\n",
    "        return ngram + [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from w2v.utils import pre_process_raw_article, mecab_tokenize\n",
    "from utils import pre_process_raw_article, mecab_tokenize\n",
    "from torch.utils.data import Dataset\n",
    "from nltk import sent_tokenize\n",
    "from abc import abstractmethod\n",
    "import pandas as pd\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "class FastTextDataset(NegSamplingDataset):\n",
    "    \"\"\"Fast Text Dataset.\n",
    "\n",
    "    Args:\n",
    "        config (dict): hyperparameters\n",
    "        word_frequency (dict): word index - word frequency map for negative sampling\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        if 'pkl' in config.file_path:\n",
    "            with open(config.file_path, 'rb') as f:\n",
    "                corpus = pickle.load(f)[:1000]\n",
    "        elif config.file_path == 'treebank':\n",
    "            corpus = treebank.sents()[:5]\n",
    "        else:\n",
    "            articles = pd.read_csv(config.file_path, encoding='utf-8')['article'].dropna().values\n",
    "            #pre process\n",
    "            corpus = self.pre_process(articles)\n",
    "            \n",
    "        ngram_corpus = self.fast_text_pre_process(corpus)\n",
    "        self.ngram_word_to_idx, self.ngram_idx_to_word, _ = self.construct_word_idx(ngram_corpus)\n",
    "        #construct word matrix\n",
    "        self.word_to_idx, self.idx_to_word, self.word_frequency = self.construct_word_idx(corpus)\n",
    "        #make dataset\n",
    "        self.x, self.y = self.construct_dataset(corpus, config)\n",
    "    \n",
    "    def fast_text_pre_process(self, corpus):\n",
    "        return [[ngram(w, 3) for w in s] for s in corpus]\n",
    "\n",
    "    def construct_word_idx(self, corpus):\n",
    "        print('constructing word matrix')\n",
    "        corpus_flatten = list(itertools.chain.from_iterable(corpus))\n",
    "        if isinstance(corpus_flatten[0], list):\n",
    "            word_frequency = collections.Counter(itertools.chain.from_iterable(corpus_flatten))\n",
    "        else:\n",
    "            word_frequency = collections.Counter(corpus_flatten)\n",
    "        word_frequency = {word: word_frequency[word]**(3/4) for idx, word in enumerate(word_frequency)}\n",
    "        word_to_idx = {word: idx for idx, word in enumerate(word_frequency)}\n",
    "        idx_to_word = {word_to_idx[word]: word for word in word_to_idx}\n",
    "\n",
    "        return word_to_idx, idx_to_word, word_frequency\n",
    "    \n",
    "    def neg_sample(self, word_contxt, config):\n",
    "        word_universe = self.word_to_idx.keys() - set(word_contxt)\n",
    "        word_distn = np.array([self.word_frequency[idx] for idx in word_universe])\n",
    "        word_distn = word_distn / word_distn.sum()\n",
    "        \n",
    "        return np.random.choice(a=list(word_universe), size=config.neg_sample_size*config.window_size*2, p=word_distn)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx): idx = idx.tolist()\n",
    "        pos = torch.Tensor([self.ngram_word_to_idx[c] for c in ngram(self.x[0][idx], 3)]).long()\n",
    "        neg = [torch.Tensor([self.ngram_word_to_idx[c] for c in ngram(w, 3)]).long() for w in self.x[1][idx]]\n",
    "        label = torch.Tensor([self.ngram_word_to_idx[c] for c in ngram(self.y[idx], 3)]).long()\n",
    "        \n",
    "#         return (pos,neg), label\n",
    "        return (self.word_to_idx[self.x[0][idx]], [self.word_to_idx[c] for c in self.x[1][idx]]), self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing word matrix\n",
      "constructing word matrix\n",
      "constructing training dataset\n"
     ]
    }
   ],
   "source": [
    "fast_txt_dataset = FastTextDataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextEmbeddingModule(nn.Module):\n",
    "    def __init__(self, idx_to_word, ngram_word_to_idx, config):\n",
    "        super().__init__()\n",
    "        self.idx_to_word = idx_to_word\n",
    "        self.ngram_word_to_idx = ngram_word_to_idx\n",
    "        self.embedding = nn.Embedding(len(self.ngram_word_to_idx), config.embed_dim).float()\n",
    "        self.embedding.weight.data.uniform_(-1,1)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        word = self.idx_to_word[x]\n",
    "        ngram_x = ngram(word, 3)\n",
    "        subind = torch.Tensor([self.ngram_word_to_idx[c] for c in ngram_x]).long()\n",
    "#         print(ngram_x[0])\n",
    "#         print(self.ngram_word_to_idx[ngram_x[0]])\n",
    "#         print(torch.Tensor([self.ngram_word_to_idx[c] for c in ngram_x]).long())\n",
    "        embedded = self.embedding(subind).view(len(subind), -1)\n",
    "        embedded.retain_grad()\n",
    "        net = embedded.mean(axis=0) #수정하기\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, [5, 21, 78, 75]), ',')\n",
      "tensor([-0.0322, -0.0222, -0.0154, -0.0116, -0.3570,  0.0091,  0.4104, -0.2832,\n",
      "        -0.0693, -0.0354, -0.2976,  0.1623,  0.1971, -0.4275,  0.3107,  0.2935,\n",
      "        -0.3346,  0.2601, -0.2888, -0.1285], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for i in fast_txt_dataset:\n",
    "    print(i)\n",
    "    print(FastTextEmbeddingModule(fast_txt_dataset.idx_to_word, fast_txt_dataset.ngram_word_to_idx, config)(i[0][0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-29b1a4059387>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mpos_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mneg_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    seqs, labels = zip(*data)\n",
    "    return seqs, labels\n",
    "\n",
    "dataloader = DataLoader(fast_txt_dataset, batch_size=config.batch_size, \\\n",
    "                        shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "target_emb = FastTextEmbeddingModule(fast_txt_dataset.idx_to_word, fast_txt_dataset.ngram_word_to_idx, config).to(config.device)\n",
    "context_emb = FastTextEmbeddingModule(fast_txt_dataset.idx_to_word, fast_txt_dataset.ngram_word_to_idx, config).to(config.device)\n",
    "sigmoid = nn.Sigmoid()\n",
    "similar = nn.CosineSimilarity()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(list(target_emb.parameters()) + list(context_emb.parameters()), lr=5e-3, momentum=0.9)\n",
    "\n",
    "for epoch in range(10):\n",
    "    print('Epoch ' + str(epoch))\n",
    "    for i, sample in enumerate(dataloader):\n",
    "        print(np.array(sample[0])[:,1].astype('float64'))\n",
    "        pos_idx = torch.Tensor(np.array(sample[0])[:,0].astype('float64')).long().to(config.device)\n",
    "        neg_idx = torch.Tensor(np.array(sample[0])[:,1].astype('float64')).long().to(config.device)\n",
    "#         neg_idx = np.array(sample[0])[:,1].to(config.device)\n",
    "        target_idx = sample[1].long().to(config.device)\n",
    "        pos_label = 1\n",
    "        neg_label = 0\n",
    "        \n",
    "        pos = context_emb(pos_idx)\n",
    "        neg = context_emb(neg_idx)\n",
    "        target = target_emb(target_idx)\n",
    "        print(pos_idx)\n",
    "#         print(pos)\n",
    "#         print(neg)\n",
    "#         print(target)[]\n",
    "#         print(pos.shape, target.shape)\n",
    "#         print(similar(pos,target))\n",
    "        pred_pos = sigmoid(similar(pos, target))\n",
    "        pred_neg = sigmoid(similar(neg, target.unsqueeze(1).expand_as(neg)))\n",
    "\n",
    "        pos_loss = criterion(pred_pos, pos_label)\n",
    "        neg_loss = torch.sum(criterion(pred_neg, neg_label.unsqueeze(1).expand_as(pred_neg)))\n",
    "\n",
    "        pos_loss.retain_grad()\n",
    "        neg_loss.retain_grad()\n",
    "        \n",
    "        loss = pos_loss + neg_loss\n",
    "        loss.retain_grad()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 99:\n",
    "            print(i, loss.item())\n",
    "            if i % 100000 == 99:\n",
    "                with open('./checkpoints/neg_sample_checkpoint_epoch' + str(epoch) + '_' + str(i) + '.pkl', 'wb') as f:\n",
    "                    pickle.dump(target_emb, f)\n",
    "                print('./checkpoints/neg_sample_checkpoint_epoch' + str(epoch) + '_' + str(i) + '.pkl saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp-embedding]",
   "language": "python",
   "name": "conda-env-nlp-embedding-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
