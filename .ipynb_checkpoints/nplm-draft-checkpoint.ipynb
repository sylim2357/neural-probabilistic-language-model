{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk import sent_tokenize\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import easydict\n",
    "import MeCab\n",
    "import torch\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'D:\\\\data\\\\text\\\\news-articles\\\\kbanker_articles_subtitles.csv'\n",
    "CONFIG_PATH = 'config.json'\n",
    "device = torch.device(\"cuda:0\")\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    args = easydict.EasyDict(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing shit\n",
    "def pre_process_raw_article(article):\n",
    "    \"\"\"Pre-processing news articles.\n",
    "    \n",
    "    Args\n",
    "        article (str): article text\n",
    "    \n",
    "    \"\"\"\n",
    "    replacements = [\n",
    "        ('[“”]', '\"'),\n",
    "        ('[‘’]', '\\''),\n",
    "        ('\\([^)]*\\)', ''),\n",
    "        ('[^가-힣\\'\"A-Za-z0-9.\\s\\?\\!]', ' '),\n",
    "        ('(?=[^0-9])\\.(?=[^0-9])', '. '),\n",
    "        ('\\s\\s+', ' ')\n",
    "    ]\n",
    "    \n",
    "    for old, new in replacements:\n",
    "        article = re.sub(old, new, article)\n",
    "        \n",
    "    return article\n",
    "\n",
    "def mecab_tokenize(sentence):\n",
    "    t = MeCab.Tagger()\n",
    "    return [re.split(',', re.sub('\\t', ',', s))[0] for s in t.parse(sentence).split('\\n') if (s!='') & ('EOS' not in s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPCorpusDataset(Dataset):\n",
    "    \"\"\"NLP Corpus dataset.\n",
    "    \n",
    "    Args:\n",
    "        csv_file (str): Path to the csv file\n",
    "        root_dir (str): root\n",
    "        \n",
    "    Attributes:\n",
    "        root_dir (str): root\n",
    "        word_to_idx (dict): word_to_idx mapping\n",
    "        idx_to_word (dict): idx_to_word mapping\n",
    "        x (list): train data (5-gram)\n",
    "        y (list): label\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir):\n",
    "        articles = pd.read_csv(csv_file, encoding='utf-8')['article'].dropna().values\n",
    "        articles = [pre_process_raw_article(article) for article in articles]\n",
    "        sentences = itertools.chain.from_iterable([sent_tokenize(article) for article in articles])\n",
    "        corpus = [mecab_tokenize(s) for s in list(sentences)]\n",
    "        self.root_dir = root_dir\n",
    "        del articles\n",
    "        del sentences\n",
    "        \n",
    "        #construct word matrix\n",
    "        word_set = set(itertools.chain.from_iterable(corpus))\n",
    "        self.word_to_idx = {word : idx for idx, word in enumerate(word_set)}\n",
    "        self.idx_to_word = {self.word_to_idx[word] : word for word in self.word_to_idx}\n",
    "        del word_set\n",
    "        corpus = [[self.word_to_idx[word] for word in sentence] for sentence in corpus]\n",
    "        \n",
    "        #make train label dataset\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        for sentence in corpus:\n",
    "            for i in range(len(sentence) - args.window_size):\n",
    "                self.x.append(sentence[i:i+args.window_size])\n",
    "                self.y.append([sentence[i+args.window_size]])\n",
    "        del corpus\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp_dataset = NLPCorpusDataset(csv_file=FILE_PATH, root_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('kbanker_nlp_dataset.pkl', 'wb') as f:\n",
    "#     pickle.dump(nlp_dataset, f)\n",
    "    \n",
    "with open('D:\\\\data\\\\text\\\\torch-dataset\\\\kbanker_nlp_dataset.pkl', 'rb') as f:\n",
    "    nlp_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModule(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, h_dim):\n",
    "        super(EmbeddingModule, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.h_dim = h_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim).float()\n",
    "        self.embedding.weight.data.uniform_(-1,1)\n",
    "        self.linear1 = nn.Linear(embed_dim*args.window_size, h_dim)\n",
    "        self.linear1.weight.data.uniform_(-1,1)\n",
    "        self.linear2 = nn.Linear(h_dim, vocab_size)\n",
    "        self.linear2.weight.data.uniform_(-1,1)\n",
    "        self.motorway = nn.Linear(embed_dim*args.window_size, vocab_size)\n",
    "        self.motorway.weight.data.uniform_(-1,1)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x).view(args.batch_size, args.window_size*self.embed_dim)\n",
    "        embedded.retain_grad()\n",
    "        net = self.linear1(embedded)\n",
    "        net = self.tanh(net)\n",
    "        net = self.linear2(net)\n",
    "        net = net + self.motorway(embedded)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "99 63.13578414916992\n",
      "199 63.04270553588867\n",
      "299 64.4983139038086\n",
      "399 64.17352294921875\n",
      "499 64.94847869873047\n",
      "599 64.23686218261719\n",
      "699 63.3019905090332\n",
      "799 62.33510208129883\n",
      "899 62.901512145996094\n",
      "999 63.828792572021484\n",
      "Epoch 1\n",
      "99 62.62953186035156\n",
      "199 62.32461929321289\n",
      "299 63.7624397277832\n",
      "399 63.61264419555664\n",
      "499 64.48085021972656\n",
      "599 63.7266845703125\n",
      "699 62.833984375\n",
      "799 61.30979919433594\n",
      "899 62.07927322387695\n",
      "999 63.38001251220703\n",
      "Epoch 2\n",
      "99 62.12385559082031\n",
      "199 61.63299560546875\n",
      "299 63.055423736572266\n",
      "399 63.054622650146484\n",
      "499 64.00579071044922\n",
      "599 63.22053146362305\n",
      "699 62.382171630859375\n",
      "799 60.31037521362305\n",
      "899 61.2607536315918\n",
      "999 62.94895935058594\n",
      "Epoch 3\n",
      "99 61.623714447021484\n",
      "199 60.984073638916016\n",
      "299 62.363380432128906\n",
      "399 62.503211975097656\n",
      "499 63.52195358276367\n",
      "599 62.722198486328125\n",
      "699 61.94243240356445\n",
      "799 59.328678131103516\n",
      "899 60.45521926879883\n",
      "999 62.51863479614258\n",
      "Epoch 4\n",
      "99 61.128238677978516\n",
      "199 60.37434387207031\n",
      "299 61.67583465576172\n",
      "399 61.95411682128906\n",
      "499 63.02989196777344\n",
      "599 62.24195098876953\n",
      "699 61.50883483886719\n",
      "799 58.3699836730957\n",
      "899 59.66128158569336\n",
      "999 62.08548355102539\n",
      "Epoch 5\n",
      "99 60.67997741699219\n",
      "199 59.79709243774414\n",
      "299 61.010353088378906\n",
      "399 61.41447830200195\n",
      "499 62.53922653198242\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    seqs, labels = zip(*data)\n",
    "    return seqs, labels\n",
    "\n",
    "dataloader = DataLoader(nlp_dataset, batch_size=args.batch_size, \\\n",
    "                        shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "model = EmbeddingModule(len(nlp_dataset.word_to_idx),\\\n",
    "                        args.embedding_dim, args.h_dim).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(args.epoch):\n",
    "    print('Epoch ' + str(epoch))\n",
    "    for i, sample in enumerate(dataloader):\n",
    "        x = torch.LongTensor(sample[0]).to(device)\n",
    "        y = torch.LongTensor(sample[1]).view(args.batch_size).to(device)\n",
    "        y_pred = model(x)\n",
    "        \n",
    "#         before = model.embedding.weight.data.cpu().numpy()\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "#         after = model.embedding.weight.data.cpu().numpy()\n",
    "        \n",
    "        if i % 100 == 99:\n",
    "            print(i, loss.item())\n",
    "#             print(model.embedding.weight.grad)\n",
    "#             print((before-after).sum())\n",
    "        \n",
    "        if i == 1000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp-embedding]",
   "language": "python",
   "name": "conda-env-nlp-embedding-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
